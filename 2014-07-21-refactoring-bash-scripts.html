---
title: 'Refactoring Bash Scripts'
layout: post
published: '2014-07-21T23:59:00-04:00'
feed: 'japh(r) by Chris Strom'
link: 'http://feedproxy.google.com/~r/JaphrByChrisStrom/~3/tRRU3dWBif4/refactoring-bash-scripts.html'
author:
    name: 'Chris Strom'
    email: noreply@blogger.com
    url: 'https://plus.google.com/117928918793969810642'
tags:
    - bash
    - benchmarking
    - chain
    - dart
    - dartlang

---

<div class=top-chain-links></div><br />I'll be honest here: I'm a pretty terribly Bash script coder. I find the <code>man</code> page too overwhelming to really get better at it. If I need to do something in Bash—even something simple like conditional statements—I grep through <code>/etc/init.d</code> scripts or fall back to the Google machine.<br /><br />But tonight, there is no hiding. I have two Bash scripts (actually, just shell scripts at this point) that do nearly the same thing: <code>benchmark.sh</code> and <code>benchmark_js.sh</code>. Both perform a series of benchmarking runs of code for <a href="http://designpatternsindart.com/">Design Patterns in Dart</a>, the idea being that it might be useful to have actual numbers to back up some of the approaches that I include in the book. But, since this is <a href="http://dartlang.org">Dart</a>, it makes sense to benchmark both on the Dart VM and on a JavaScript VM (the latter because most Dart code will be compiled with <code>dart2js</code>). The two benchmark shell scripts are therefore responsible for running and generating summary results for Dart and JavaScript.<br /><br />The problem with two scripts is twofold. First, I need to keep them in sync—any change made to one needs to go into the other. Second, if I want to generalize this for any design pattern, I have hard-coded way too much in both scripts. To the refactoring machine, Robin!<br /><br />To get an idea where to start, I <code>diff</code> the two scripts. I have been working fairly hard to keep the two scripts in sync, so there are only two places that differ. The JavaScript version includes a section that compiles the Dart benchmarks in to JavaScript:<pre class=prettyprint>$ diff -u1 tool/benchmark.sh tool/benchmark_js.sh <br />--- tool/benchmark.sh   2014-07-21 22:32:44.047778498 -0400<br />+++ tool/benchmark_js.sh        2014-07-21 20:54:20.803634500 -0400<br />@@ -11,2 +11,16 @@<br /> <br /><b>+# Compile<br />+wrapper='function dartMainRunner(main, args) { main(process.argv.slice(2)); }';<br />+dart2js -o tool/benchmark.dart.js \<br />+           tool/benchmark.dart<br />+echo $wrapper >> tool/benchmark.dart.js<br />+<br />+dart2js -o tool/benchmark_single_dispatch_iteration.dart.js \<br />+           tool/benchmark_single_dispatch_iteration.dart<br />+echo $wrapper >> tool/benchmark_single_dispatch_iteration.dart.js<br />+<br />+dart2js -o tool/benchmark_visitor_traverse.dart.js \<br />+           tool/benchmark_visitor_traverse.dart<br />+echo $wrapper >> tool/benchmark_visitor_traverse.dart.js</b><br />+<br />...</pre>The other difference is actually running the benchmarks—the JavaScript version needs to run through <a href="http://nodejs.com/">node.js</a>:<pre class=prettyprint>$ diff -u1 tool/benchmark.sh tool/benchmark_js.sh <br />--- tool/benchmark.sh   2014-07-21 22:32:44.047778498 -0400<br />+++ tool/benchmark_js.sh        2014-07-21 20:54:20.803634500 -0400<br />...<br />@@ -15,7 +29,7 @@<br /> do<br />-    ./tool/benchmark.dart --loop-size=$X \<br /><b>+    node ./tool/benchmark.dart.js --loop-size=$X \</b><br />         | tee -a $RESULTS_FILE<br />-    ./tool/benchmark_single_dispatch_iteration.dart --loop-size=$X \<br /><b>+    node ./tool/benchmark_single_dispatch_iteration.dart.js --loop-size=$X \</b><br />         | tee -a $RESULTS_FILE<br />-    ./tool/benchmark_visitor_traverse.dart --loop-size=$X \<br /><b>+    node ./tool/benchmark_visitor_traverse.dart.js --loop-size=$X \</b><br />         | tee -a $RESULTS_FILE</pre>For refactoring purposes, I start with the latter difference. The former is a specialization that can be performed in a single conditional. The latter involves both uses of the script.<br /><br />That will suffice for initial strategy, what about tactics? Glancing at the Dart <code>benchmark.sh</code> script, I see that I have a structure that looks like:<pre class=prettyprint>#!/bin/bash<br /><br /><b>RESULTS_FILE=tmp/benchmark_loop_runs.tsv<br />SUMMARY_FILE=tmp/benchmark_summary.tsv<br />LOOP_SIZES="10 100 1000 10000 100000"</b><br /><br /><b># Initialize artifact directory</b><br />...<br /><br /><b># Individual benchmark runs of different implementations</b><br />...<br /><br /><b># Summarize results</b><br />...<br /><br /><b># Visualization ready</b><br />...<br /></pre>Apparently I have been quite fastidious about commenting the code because those code section comments are actually there. They look like a nice first pass a series of functions. Also of note here is that the script starts by setting some global settings, which seems like a good idea even after refactoring—I can use these and others to specify output filenames, benchmark scripts, and whether or not to use the JavaScript VM.<br /><br />But first things first, extracting the code in each of those comment sections out into functions. I make a top-level function that will invoke all four functions-from-comment-sections:<pre class=prettyprint>_run_benchmarks () {<br />    initialize<br />    run_benchmarks<br />    summarize<br />    all_done<br />}</pre>Then I create each function as:<pre class=prettyprint># Initialize artifact directory<br />initialize () {<br />  # ...<br />}<br /><br /># Individual benchmark runs of different implementations<br />run_benchmarks () {<br />  # ...<br />}<br /><br /># Summarize results<br />summarize () {<br />  # ...<br />}<br /><br /># Visualization ready<br />all_done () {<br />  # ...<br />}<br /></pre>Since each of those sections is relying on top-level global variables, this just works™ without any additional work from me.<br /><br />Now for some actual refactoring. One of the goals here is to be able to use this same script not only for JavaScript and Dart benchmarking of the same pattern, but also for different patterns. To be able to use this for different patterns, I need to stop hard-coding the scripts inside the new <code>run_benchmarks</code> function:<pre class=prettyprint>run_benchmarks () {<br />    echo "Running benchmarks..."<br />    for X in 10 100 # 1000 10000 100000<br />    do<br /><b>      ./tool/benchmark.dart --loop-size=$X \<br />          | tee -a $RESULTS_FILE<br />      ./tool/benchmark_single_dispatch_iteration.dart --loop-size=$X \<br />          | tee -a $RESULTS_FILE<br />      ./tool/benchmark_visitor_traverse.dart --loop-size=$X \<br />          | tee -a $RESULTS_FILE</b><br />    done<br />    echo "Done. Results stored in $results_file."<br />}<br /></pre>The only thing that is different between those three implementation benchmarks is the name of the benchmark file. So a list of files in a global variable that could be looped over is my next step:<pre class=prettyprint>BENCHMARK_SCRIPTS=(<br />    tool/benchmark.dart<br />    tool/benchmark_single_dispatch_iteration.dart<br />    tool/benchmark_visitor_traverse.dart<br />)</pre>Up until this point, I think I could get away with regular Bourne shell scripting, but lists like this are only available in Bash. With that, I can change <code>run_benchmarks</code> to:<pre class=prettyprint>run_benchmarks () {<br />    echo "Running benchmarks..."<br />    for X in 10 100 1000 10000 100000<br />    do<br /><b>        for script in ${BENCHMARK_SCRIPTS[*]}<br />        do<br />            ./$script --loop-size=$X | tee -a $results_file<br />        done</b><br />    done<br />    echo "Done. Results stored in $results_file."<br />}</pre>At this point, I would like to get a feel for what the common part of the script is and what specialized changes are needed for each new pattern benchmark. So I move all of my new functions out into a <code>_benchmark.sh</code> script that can be ”sourced” by the specialized code:<pre class=prettyprint>#!/bin/bash<br /><br /><b>source ./tool/_benchmark.sh</b><br /><br />BENCHMARK_SCRIPTS=(<br />    tool/benchmark.dart<br />    tool/benchmark_single_dispatch_iteration.dart<br />    tool/benchmark_visitor_traverse.dart<br />)<br /><br />RESULTS_FILE=benchmark_loop_runs.tsv<br />SUMMARY_FILE=benchmark_summary.tsv<br /><br />_run_benchmarks</pre>That is pretty nice. I can easily see how I would use this for other patterns—for each implementation being benchmarked, I would simple add them to the list of <code>BENCHMARK_SCRIPTS</code><br /><br /><br /><br /><span style="color: #ccc">Day #129</span>  <br /><br /><p class=bottom-chain-links><a href="http://japhr.blogspot.com/2014/07/reading-files-from-stdin-in-dart.html">&lsaquo;prev</a> | <a href="http://japhr.blogspot.com/2013/11/new-chain-patterns-in-polymer.html">My Chain</a> | <span style="color: #ccc">next&rsaquo;</span> </p><script>var b_links = document.getElementsByClassName('bottom-chain-links'),      t_links = document.getElementsByClassName('top-chain-links');  if (b_links.length == 1 && t_links.length == 1) {   t_links[0].innerHTML = b_links[0].innerHTML; } </script><img src="http://feeds.feedburner.com/~r/JaphrByChrisStrom/~4/tRRU3dWBif4" height="1" width="1"/>
